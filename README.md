Speech emotion Recognition

Drive Link For Dataset:https://drive.google.com/drive/folders/12Ta0BAagYOgRP63ezmyVhXCEQHPEztFd?usp=sharing

Link to run the project on local machine : https://studio.edgeimpulse.com/public/340495/live


edge impulse:https://smartphone.edgeimpulse.com/classifier.html

Dataset contains :

1. RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song):
The dataset contains 7356 files with speech and song recordings of 24 professional actors performing in 8 different emotions (neutral, calm, happy, sad, angry, fearful, surprised, and disgusted). It includes both acted and improvised speech and song.
2. TESS (Toronto Emotional Speech Set):
The dataset consists of 2800 files with naturalistic speech recordings performed by two actors (one male, one female), expressing 7 different emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral).
3. SAVEE (Surrey Audio-Visual Expressed Emotion):
The dataset contains 480 files of acted speech by 4 male speakers, expressing 4 different emotions (neutral, happy, angry, and sad) with varying levels of intensity.

Speech Emotion Recognition System is a tool that classifies speech based on the emotions conveyed in the speech. The system categorizes speech into four primary emotional states: Angry, Sad, Happy, and Surprised.

Challenges Faced:

1) Data Collection: One of the primary challenges was collecting a sufficient amount of labeled data that was accurately classified into the four mentioned emotional classes. Initially, the team attempted to record their own samples, but due to limited resources, they could only gather 100 samples. This led to overfitting issues when training the model.

2) Language Limitation: The model was trained specifically for English speech, making it challenging to classify voices of different languages accurately.

Future Extensions:

1) Cloud Computing Integration: To handle large-scale speech processing, the team plans to integrate cloud computing technology. This would involve offloading the computational load to cloud servers like Amazon Web Services (AWS) or Google Cloud Platform (GCP), enabling real-time processing of large volumes of audio data.

2) Multilingual Support: Expanding the system to support multiple languages would increase its usability and applicability. This could involve training the model on datasets in different languages and implementing language detection to switch between models based on the language of the input speech.


Scan QR for Live Testing on device 

<p align="center">
  <img width="200" height="auto" src="images/Output.png" alt="output">
</p>



deployed kaggle link:https://www.kaggle.com/code/siddharthpilkhn/notebooka4d73bba03

3) Emotion Analysis in Education: The system could be integrated into educational settings to analyze the emotional states of students and teachers during classes. This could provide insights for improving teaching methods and student engagement.

